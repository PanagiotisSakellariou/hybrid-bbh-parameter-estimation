{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58c0ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10448672",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 99\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c0ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterTensorFlowToPyTorchDataset(IterableDataset):\n",
    "    def __init__(self, tf_dataset_path):\n",
    "        \"\"\"\n",
    "        A PyTorch dataset that fetches data from a TensorFlow dataset.\n",
    "        \n",
    "        Args:\n",
    "        - tf_dataset: A TensorFlow dataset.\n",
    "        \"\"\"\n",
    "        self.tf_path = tf_dataset_path\n",
    "        self.tf_dataset = tf.data.Dataset.load(tf_dataset_path)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return tf.data.experimental.cardinality(self.tf_dataset).numpy()\n",
    "            \n",
    "    def __iter__(self):\n",
    "        for element in self.tf_dataset.as_numpy_iterator():\n",
    "            features, labels = element\n",
    "            features = torch.tensor(features, dtype=torch.float32).view(1, -1)  # Reshape to match model input shape\n",
    "            labels = torch.tensor(labels, dtype=torch.float32)\n",
    "            \n",
    "            # # Normalize the labels\n",
    "            # labels = self.output_normalization(labels)\n",
    "    \n",
    "            yield features, labels\n",
    "    \n",
    "    def size(self, in_gb=False):\n",
    "        ''' Returns the size of the Dataset in MB or GB.\n",
    "        '''\n",
    "        if os.path.exists(self.tf_path):\n",
    "            if os.path.isfile(self.tf_path):\n",
    "                # If it's a single file\n",
    "                size_in_bytes = os.path.getsize(self.tf_path)\n",
    "            elif os.path.isdir(self.tf_path):\n",
    "                # If it's a directory, sum up the sizes of all files inside it\n",
    "                size_in_bytes = sum(\n",
    "                    os.path.getsize(os.path.join(root, file))\n",
    "                    for root, _, files in os.walk(self.tf_path)\n",
    "                    for file in files\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Path '{self.tf_path}' is neither a file nor a directory.\")\n",
    "            \n",
    "                            \n",
    "            if in_gb == False:\n",
    "                size_in_mb = size_in_bytes / (1024*1024) # Convert to MB\n",
    "                return size_in_mb\n",
    "            else:\n",
    "                size_in_gb = size_in_bytes / (1024*1024*1024) # Convert to GB\n",
    "                return size_in_gb\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Data file not found at {self.tf_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6584741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep model\n",
    "class DeepModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep CNN model for regression.\n",
    "    \n",
    "    The model requires initialization before loading weights. \n",
    "    To initialize the model, call model.initialize(input_tensor).\n",
    "    \"\"\"\n",
    "    def __init__(self, num_vars):\n",
    "        super(DeepModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=16, stride=1, dilation=1)\n",
    "        # self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=16, stride=1, dilation=2)\n",
    "        # self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=16, stride=1, dilation=2)\n",
    "        # self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        \n",
    "        self.conv4 = nn.Conv1d(256, 512, kernel_size=32, stride=1, dilation=2)\n",
    "        # self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = None # This layer will be initialized in the forward method\n",
    "        # self.drp1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        # self.drp2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(64, num_vars)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        # x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        # x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        # x = self.bn3(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        # x = self.bn4(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Initialize the first fully connected layer the first time forward is run\n",
    "        if self.fc1 is None:\n",
    "            self.fc1 = nn.Linear(x.size(1), 128).to(x.device)\n",
    "            \n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        # x = self.drp1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = torch.relu(x)\n",
    "        # x = self.drp2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def initialize(self, x):\n",
    "        \"\"\"Initialize the model by passing an input tensor.\"\"\"\n",
    "        self.forward(x)\n",
    "        print(\"\\nModel initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d9a534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Deep Model with CNN and Transformer Encoder\n",
    "class HybridDeepModel(nn.Module):\n",
    "    def __init__(self, num_variables):\n",
    "        super(HybridDeepModel, self).__init__()\n",
    "        \n",
    "        # CNN Layers\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=16, stride=1, dilation=1)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=16, stride=1, dilation=2)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=16, stride=1, dilation=2)\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "        \n",
    "        self.conv4 = nn.Conv1d(256, 512, kernel_size=32, stride=1, dilation=2)\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=4, stride=4)\n",
    "\n",
    "        # Normalization before Transformer\n",
    "        self.norm = nn.LayerNorm(512)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=4, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        self.fc1 = nn.Linear(512, 128)  # Adjust input size based on the output of the last conv layer\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, num_variables)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # CNN Feature Extraction\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = torch.relu(x)   \n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.pool4(x) \n",
    "        \n",
    "        # Prepare for Transformer\n",
    "        # Transpose to match Encoder input shape: (batch_size, sequence_length, embed_dim)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        x = self.transformer(x)  # Output: (sequence_length, batch_size, embed_dim)\n",
    "        x = x[:, -1, :]  # Take the last sequence element (batch_size, embed_dim)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def initialize(self, x):\n",
    "        \"\"\"Initialize the model by passing an input tensor.\"\"\"\n",
    "        self.forward(x)\n",
    "        print(\"Model initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e12b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your CSV\n",
    "df = pd.read_csv('./real_gw_slice_analysis/real_gw_slice_summary.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c8c1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by model and slice_type to compute mean MAE Loss and std.\n",
    "df['slice_type'] = df['slice_name'].apply(lambda x: 'reverse' if 'rev' in x else 'normal')\n",
    "\n",
    "stats = df.groupby(['model_name', 'slice_type'])['best_validation_loss'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385dbb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by model and slice_type to compute mean MAE Loss and std for boundary approach.\n",
    "bound_stats = df.groupby(['model_name', 'slice_type'])['boundary_test_loss'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "print(bound_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2bc421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on real GW data for all slices and both models for supplementary material\n",
    "slice_names = ['slice1', 'slice2', 'slice3', 'slice4', 'slice5',\n",
    "               'slice1_reverse', 'slice2_reverse', 'slice3_reverse', 'slice4_reverse', 'slice5_reverse']\n",
    "\n",
    "for model_name in ['DeepModel', 'HybridDeepModel']:\n",
    "    for slice_name in slice_names:\n",
    "        path = '/home/sakellariou/hero_disk/test/'  # <--- Change this to the path of the dataset\n",
    "        csv_path = path + 'real_gw.csv'\n",
    "\n",
    "        real_train_dataset = IterTensorFlowToPyTorchDataset(path + f'{slice_name}_train_dataset')\n",
    "        # print('\\nNumber of training samples in real dataset:', len(real_train_dataset))\n",
    "        # print(f'\\nSize of real training dataset: {real_train_dataset.size(in_gb=False):.3f} MB')\n",
    "\n",
    "        real_test_dataset = IterTensorFlowToPyTorchDataset(path + f'{slice_name}_test_dataset')\n",
    "        # print('\\nNumber of test samples in real dataset:', len(real_test_dataset))\n",
    "        # print(f'\\nSize of real test dataset: {real_test_dataset.size(in_gb=False):.3f} MB')\n",
    "\n",
    "        # Create DataLoader for batching\n",
    "        # print('\\nCreating DataLoaders...')\n",
    "        batch = 1024\n",
    "        real_train_loader = DataLoader(real_train_dataset, batch_size=batch, shuffle=False)\n",
    "        real_test_loader = DataLoader(real_test_dataset, batch_size=batch, shuffle=False)\n",
    "        \n",
    "        # Initialize model, loss function, and optimizer\n",
    "        num_vars = 3\n",
    "        models = {\n",
    "            'DeepModel': DeepModel,\n",
    "            'HybridDeepModel': HybridDeepModel\n",
    "        }\n",
    "\n",
    "        model = models[model_name](num_vars).to(device)\n",
    "        \n",
    "        weights_path = f'./real_gw_slice_analysis/{slice_name}_{model.__class__.__name__}_results/{model.__class__.__name__}_best_model.pth' # <-- Change this to the path of the best model weights\n",
    "\n",
    "\n",
    "        # Load the best model weights\n",
    "        print('\\nLoading best model weights...')\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(f'{weights_path}'))\n",
    "            print('\\nModel weights loaded successfully.')\n",
    "        except:\n",
    "            print('\\nError loading model weights. Initializing model...')\n",
    "            with torch.no_grad():\n",
    "                inputs, targets = next(iter(real_train_loader))\n",
    "                inputs = inputs.to(device)\n",
    "                model.initialize(inputs)\n",
    "            model.load_state_dict(torch.load(f'{weights_path}'))\n",
    "            print('\\nModel weights loaded successfully.')\n",
    "    \n",
    "    \n",
    "        model.eval()\n",
    "        \n",
    "        # import parameters from csv\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df['keys'] = df['row_key'].astype(str).apply(lambda x: x.split('_', 1)[1])\n",
    "        df.set_index('keys', inplace=True)\n",
    "        \n",
    "        # Per-value accumulators\n",
    "        mass_1_index = 0  # Mass 1 is the first variable (index 0)\n",
    "        mass_2_index = 1  # Mass 2 is the second variable (index 1)\n",
    "        distance_index = 2  # Distance is the third variable (index 2)\n",
    "        \n",
    "        mae_sum = np.zeros(num_vars)\n",
    "        bound_mae_sum = np.zeros(num_vars)\n",
    "        count = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            progress_bar = tqdm(real_test_loader, desc=\"Evaluating\", dynamic_ncols=True)\n",
    "\n",
    "            for inputs, targets in progress_bar:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                test_predictions = model(inputs)\n",
    "\n",
    "                y_true = targets.cpu().numpy().astype(np.float64)\n",
    "                y_pred = test_predictions.cpu().numpy().astype(np.float64)\n",
    "\n",
    "                # Adjust predictions based on the parameters from the CSV file\n",
    "                keys = [f'{m1:.2f}_{m2:.2f}_{d:.1f}' for m1, m2, d in zip(y_true[:, 0].round(2), y_true[:, 1].round(2), y_true[:, 2].round(1))]\n",
    "\n",
    "                df_batch = df[df.index.isin(keys)].reindex(keys)\n",
    "                \n",
    "                mae_sum += np.sum(np.abs(y_true - y_pred), axis=0)\n",
    "                count += y_true.shape[0]\n",
    "                \n",
    "                # Adjust predictions based on the bounds on the parameters from the CSV file\n",
    "                #-------------------------------------------------------------------\n",
    "                # print(df_batch)  # Debugging line to check the batch DataFrame\n",
    "                m1_low = df_batch['mass_1'].values + df_batch['mass_1_low'].values\n",
    "                m1_high = df_batch['mass_1'].values + df_batch['mass_1_high'].values\n",
    "                m2_low = df_batch['mass_2'].values + df_batch['mass_2_low'].values\n",
    "                m2_high = df_batch['mass_2'].values + df_batch['mass_2_high'].values\n",
    "                distance_low = df_batch['distance'].values + df_batch['distance_low'].values\n",
    "                distance_high = df_batch['distance'].values + df_batch['distance_high'].values\n",
    "                \n",
    "                mass1 = y_true[:, mass_1_index]\n",
    "                mass2 = y_true[:, mass_2_index]\n",
    "                distance = y_true[:, distance_index]\n",
    "\n",
    "                pr_mass1 = y_pred[:, mass_1_index]\n",
    "                pr_mass2 = y_pred[:, mass_2_index]\n",
    "                pr_distance = y_pred[:, distance_index]\n",
    "\n",
    "                in_m1 = (pr_mass1 >= m1_low) & (pr_mass1 <= m1_high)\n",
    "                in_m2 = (pr_mass2 >= m2_low) & (pr_mass2 <= m2_high)\n",
    "                in_distance = (pr_distance >= distance_low) & (pr_distance <= distance_high)\n",
    "                \n",
    "                adjusted_pred = y_pred.copy()\n",
    "                # adjusted_pred[in_all] = y_true[in_all]  # Adjust predictions to match true values where conditions are met\n",
    "                adjusted_pred[in_m1, mass_1_index] = y_true[in_m1, mass_1_index]\n",
    "                adjusted_pred[in_m2, mass_2_index] = y_true[in_m2, mass_2_index]\n",
    "                adjusted_pred[in_distance, distance_index] = y_true[in_distance, distance_index]\n",
    "                \n",
    "                bound_mae_sum += np.sum(np.abs(y_true - adjusted_pred), axis=0)\n",
    "                #-------------------------------------------------------------------\n",
    "                \n",
    "                m1 = y_true[:, mass_1_index]\n",
    "                m1_low_value = df_batch['mass_1_low'].values\n",
    "                m1_high_value = df_batch['mass_1_high'].values\n",
    "                m2 = y_true[:, mass_2_index]\n",
    "                m2_low_value = df_batch['mass_2_low'].values\n",
    "                m2_high_value = df_batch['mass_2_high'].values\n",
    "                d = y_true[:, distance_index]\n",
    "                d_low_value = df_batch['distance_low'].values\n",
    "                d_high_value = df_batch['distance_high'].values\n",
    "                pr_m1 = y_pred[:, mass_1_index]\n",
    "                pr_m2 = y_pred[:, mass_2_index]\n",
    "                pr_d = y_pred[:, distance_index]\n",
    "                event_names = df_batch['event_name'].values\n",
    "                \n",
    "                print(f'{model_name} - {slice_name}:')\n",
    "                for i in range (len(m1)):\n",
    "                    print(f'\\nEvent: {event_names[i]}')\n",
    "                    print(f'Mass 1 true: {m1[i]} lower: {m1_low_value[i]} upper: {m1_high_value[i]} predicted: {pr_m1[i]}')\n",
    "                    print(f'Mass 2 true: {m2[i]} lower: {m2_low_value[i]} upper: {m2_high_value[i]} predicted: {pr_m2[i]}')\n",
    "                    print(f'Distance true: {d[i]} lower: {d_low_value[i]} upper: {d_high_value[i]} predicted: {pr_d[i]}')\n",
    "                print()\n",
    "            \n",
    "            # Final metrics\n",
    "            mae = mae_sum / count\n",
    "            bound_mae = bound_mae_sum / count\n",
    "            total_mae_loss = np.sum(mae) / num_vars\n",
    "            total_bound_mae_loss = np.sum(bound_mae) / num_vars\n",
    "            print(f'{model_name} - {slice_name} Test Loss: {total_mae_loss:.4f} Bound Test Loss: {total_bound_mae_loss:.4f}')\n",
    "            print('-------------------------------------------------------------------')\n",
    "    \n",
    "\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lite_grav",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
